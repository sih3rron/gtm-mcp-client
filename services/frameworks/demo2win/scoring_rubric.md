# Demo2Win - Scoring Rubric

## Overall Score Interpretation

**9–10 (Exceptional)**  
- Masterful execution across all Demo2Win components  
- Seamless narrative from opening through closing  
- Strong customer engagement, clear business impact  
- Demo drives urgency and next steps naturally  

**7–8 (Strong)**  
- Solid execution with minor gaps  
- Most components delivered effectively  
- Business outcomes clear, engagement strong  
- A few missed opportunities for impact  

**5–6 (Adequate)**  
- Core Demo2Win structure followed but lacks depth  
- Some components handled generically  
- Value connection present but not compelling  
- Engagement uneven; improvement needed  

**3–4 (Poor)**  
- Major gaps in Demo2Win execution  
- Little alignment to customer outcomes  
- Low engagement or confusing narrative  
- Lacks structure and flow  

**1–2 (Critical)**  
- Demo2Win elements mostly missing  
- No clear storyline, value, or next steps  
- Customer disengaged or confused  
- Fundamental rework required

---

## Component-Level Scoring Guidelines

### 1. Opening & Context Setting (Weight: 10%)

**Excellent (9–10)**  
- Clear, concise opening hook (<60 sec) tailored to personas  
- Agenda tied to business outcomes, not features  
- Scope, timing, and relevance confirmed  
- Stakeholders engaged immediately

**Good (7–8)**  
- Clear agenda and business framing with minor gaps  
- Hook is mostly relevant but could be sharper  
- Confirmation questions asked

**Fair (5–6)**  
- Generic opening, weak personalization  
- Agenda vague or overly product-centric  
- Minimal stakeholder confirmation

**Poor (1–4)**  
- “Jump into the demo” with no context  
- No framing or audience alignment  
- Missed opportunity to earn attention

**Key Evidence:** Hook relevance, clarity of scope, persona alignment, stakeholder confirmation.

---

### 2. Persona Recap & Success Criteria (Weight: 10%)

**Excellent (9–10)**  
- Crisp recap of roles, goals, KPIs, and success criteria  
- Stakeholder ownership and metrics referenced  
- Customer confirms accuracy

**Good (7–8)**  
- Strong recap of most roles/goals, some minor gaps  
- Confirmation received but not all metrics captured

**Fair (5–6)**  
- Partial recap with generic or vague metrics  
- Limited confirmation or unclear ownership

**Poor (1–4)**  
- No meaningful recap  
- Assumptions made, no customer validation

**Key Evidence:** Explicit metric references, stakeholder names/roles, confirmation questions.

---

### 3. Scenario Storyline & Selection (Weight: 10%)

**Excellent (9–10)**  
- Focused on 1–3 high-impact scenarios  
- Logical narrative flow with clear transitions  
- Customer priorities drive the storyline  
- Signposting used effectively

**Good (7–8)**  
- Mostly strong scenario selection and flow  
- Some transitions could be tighter  
- Generally relevant to stakeholders

**Fair (5–6)**  
- Multiple scenarios but weak prioritization  
- Occasional jumps or disjointed flow  
- Some generic examples

**Poor (1–4)**  
- Unstructured product tour  
- No clear narrative or prioritization

**Key Evidence:** Scenario selection rationale, logical flow, narrative clarity.

---

### 4. Tell–Show–Tell Execution (Weight: 25%)

**Excellent (9–10)**  
- Each segment follows Demo2Win structure:  
  - **Tell:** Setup tied to business need  
  - **Show:** Clean, minimal-click demo path  
  - **Tell:** Clear value recap (metrics, outcomes, urgency)  
- Customer language used throughout  
- Segments flow seamlessly

**Good (7–8)**  
- Clear T–S–T used in most scenarios  
- Minor gaps in setup or value recap  
- Mostly clean demo paths

**Fair (5–6)**  
- Inconsistent use of T–S–T  
- Some feature dumping or unclear value recap  
- Click paths a bit messy

**Poor (1–4)**  
- No T–S–T structure  
- Aimless clicking, feature lists, or jargon  
- No value tie-back

**Key Evidence:** Setup clarity, demo focus, final value articulation, customer engagement.

---

### 5. Interactivity & Control (Weight: 10%)

**Excellent (9–10)**  
- Well-timed check-ins throughout  
- Tangents handled via **acknowledge–clarify–redirect**  
- Customer engaged, but rep keeps control  
- Objections surfaced and managed smoothly

**Good (7–8)**  
- Good level of interactivity and control  
- Some missed opportunities to redirect or probe

**Fair (5–6)**  
- Reactive engagement  
- Occasional derailment or lost flow

**Poor (1–4)**  
- No planned interaction  
- Tangents derail demo, control lost

**Key Evidence:** Check-in moments, objection handling, flow control.

---

### 6. Proof & Risk Handling (Weight: 10%)

**Excellent (9–10)**  
- Relevant proof points (customer stories, metrics) delivered naturally  
- Top risks proactively addressed (e.g. security, integration, scale)  
- Builds credibility and de-risks decision

**Good (7–8)**  
- Good proof usage, some risk coverage  
- Minor gaps in relevance or timing

**Fair (5–6)**  
- Generic proof points, minimal risk discussion  
- Some credibility but not tailored

**Poor (1–4)**  
- No proof or irrelevant stories  
- Risks unaddressed, leaving uncertainty

**Key Evidence:** Industry-matched references, quantified outcomes, risk coverage.

---

### 7. Environment Readiness (Weight: 5%)

**Excellent (9–10)**  
- Clean demo environment, pre-staged data, correct roles  
- No distractions (notifications, lag, errors)  
- Backup plan available

**Good (7–8)**  
- Minor hiccups handled smoothly  
- Mostly polished environment

**Fair (5–6)**  
- Noticeable distractions or setup issues  
- Demo flow impacted but recovered

**Poor (1–4)**  
- Messy environment, technical issues derail flow  
- Unprofessional impression

**Key Evidence:** Data relevance, performance, visual clarity, preparedness.

---

### 8. Closing & Mutual Action Plan (Weight: 20%)

**Excellent (9–10)**  
- Recap of priorities and outcomes delivered succinctly  
- Specific **next steps with owners and dates** agreed  
- Mutual Action Plan (MAP) built live  
- Clear momentum to decision

**Good (7–8)**  
- Clear next
