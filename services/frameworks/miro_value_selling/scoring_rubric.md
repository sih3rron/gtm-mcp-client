Score Interpretation (Overall)

9–10 Exceptional – Outcomes and capabilities crisply aligned; strong measurement; compelling proof; customer momentum.
7–8 Strong – Solid execution with minor gaps; clear path to value.
5–6 Adequate – Basics covered but outcomes/capabilities/metrics need tightening.
3–4 Poor – Major gaps; unclear business value.
1–2 Critical – Framework largely missing.

Component Rubrics (weights)
Before Scenario (10%)

Excellent (9–10): Specific processes, tools, stakeholders, and baselines; customer confirms.

Good (7–8): Most details present; some gaps.

Fair (5–6): High-level outline.

Poor (1–4): Vague/assumed.

Evidence: concrete process steps, tool names, cadence, org roles.

Negative Consequences (10%)

Excellent: Quantified time/cost/risk/experience impact with examples.

Good: Clear impacts; some quantification.

Fair: General pains.

Poor: No business linkage.

Evidence: delays, meeting load, travel costs, SaaS duplication, shadow IT, burnout.

After Scenario (10%)

Excellent: Practical target state tied to initiatives; early wins identified.

Good: Compelling but partial.

Fair: Aspirational only.

Poor: None.

Evidence: async/hybrid shifts, unified visibility, cross-geo collaboration.

Positive Business Outcomes (25%)

Excellent: 2–3 prioritized Measurable Business Outcomes with thresholds/timeframes.

Good: Relevant, measurable outcomes without full prioritization.

Fair: Outcomes named but vague.

Poor: Generic benefits.

Evidence: time-to-market, meeting effectiveness, travel/SaaS cost reduction, adoption, eNPS.

Required Capabilities (25%)

Excellent: Minimal, testable capabilities mapped one-to-one to each PBO; phase plan.

Good: Clear capabilities with partial mapping.

Fair: List exists but not validated or testable.

Poor: Features unrelated to outcomes.

Evidence: unified workspace; async/hybrid facilitation; 2-way Jira/ADO; 130+ integrations; admin & analytics; Enterprise Guard; accessibility; large-scale performance.

Metrics / MBOs (10%)

Excellent: Baselines, targets, owners, cadence; early-win indicators.

Good: Metrics with partial baselines/ownership.

Fair: General KPIs only.

Poor: None.

Evidence: concrete numbers and dates; dashboard alignment.

Proof Points (10%)

Excellent: Outcome- and capability-matched stories; customer acknowledges relevance; offers reference.

Good: Relevant but not tightly matched.

Fair: Generic examples.

Poor: None.

Evidence: named customers, quantified results, third-party validation.

Scoring Method

Weighted average using component weights above.
Documentation standard: Scores ≥7 require customer quotes or concrete call evidence; all outcome/capability claims must be traceable to customer statements.
Consistency checks: High PBO scores should correlate with strong Required Capabilities and Metrics.

Common Pitfalls & QA

Over-scoring: crediting rep assertions without customer validation; feature dumps.

Under-scoring: penalizing natural conversation or minor wording differences.

QA checklist:

 2–3 prioritized PBOs in customer language

 Capabilities mapped to each PBO (Phase 1 vs. later)

 Metrics with baselines, targets, owners, cadence

 Proof points relevant to industry/outcomes

 Next steps and stakeholders captured